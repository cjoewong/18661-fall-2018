<head>
    <meta charset="utf-8">
<title> CS260 ML Algorithms </title>
<meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="shortcut icon" href="../../docs-assets/ico/favicon.png">


    <!-- Bootstrap core CSS -->
    <link href="bootstrap.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="fall2018.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->
</head>

<body>
    <div class="container">
	 <div class="panel-footer clearfix">
        <div class="pull-left">
        
            <h1>18-461/18-661: Intro to ML for Engineers</h1>
            <p class="lead">Instructors:</p><p><a href=https://www.andrew.cmu.edu/user/cjoewong/ target="_blank">Prof. Carlee Joe-Wong</a> (SV)<br /><a href="http://users.ece.cmu.edu/~smithv/" target="_blank">Prof. Virginia Smith</a> (Pitt)</p>
            <p class="lead">Lecture:</p>
            <p>SV: Tues/Thurs 9:00am-10:20am PT, B23 109/110 
            <br />Pitt: Tues/Thurs 12:00pm-1:20pm ET, HH 1107</p>
            <p class="lead">Discussion:</p>
            <p>SV: Fri 2:30pm-3:50pm PT, B23 211
            <br />Pitt: Fri 12:00pm-1:20pm, SH 125</p>
            </div>
        </div>

        <div class="main">
        <br>
        <h3>Course Overview</h3>
        <p class="p1">This course provides an introduction to machine learning and statistical pattern recognition. We will cover approaches for supervised learning (linear models, kernel methods, decision trees, neural networks) and unsupervised learning (clustering, dimensionality reduction), as well as theoretical foundations of machine learning (learning theory, optimization). Evaluation will consist of mathematical problem sets and programming projects covering a variety of real-world applications.
        </p>

        <h3>Prerequisites</h3>
        <p>This course is intended for graduate students and qualified undergraduate students with a strong mathematical and programming background. Undergraduate level training or coursework in algorithms, linear
        algebra, calculus, probability, and statistics is suggested. A background in programming will also be necessary for the problem sets; students are expected to be familiar with python or learn it during the course. At CMU, this course is most similar to MLD's 10-701, though this course is meant specifically for students in engineering.</p>
    
        <h3>Acknowledgments</h3>
        <p> This course is based in part on material developed by <a href=http://www-bcf.usc.edu/~feisha/ target="_blank">Fei Sha</a> and <a href="https://www.cs.cmu.edu/~atalwalk/" target="_blank">Ameet Talwalkar</a>.
        <hr>


        <h3>Syllabus (Subject to Change)</h3>
        <div class="bs-example">
            <table class="table">
                <thead>
                    <tr>
                        <th>Week</th>
                        <th>Topics</th>
                        <!--<th>Reading</th>
                        <th>HW</th>-->
                    </tr>
                </thead>
                <tbody>
                    <tr class="active">
                        <td>0</td>
                        <td>
                        Intro to: Intro to ML
                        </td>
                    </tr>
                    <tr class="success">
                        <td>1</td>
                        <td>
                        Math Bootcamp: Linear Algebra, Probability, Calculus, Basics of Learning Theory and Optimization
                        </td>
                    </tr>
                    <tr class="active">
                        <td>2-4</td>
                        <td>Supervised Learning, Part I: K-nearest Neighbors, Naive Bayes, Decision Trees, Linear Regression, Logistic Regression
                        </td>
                    </tr>
                    <tr class="success">
                        <td>5-6</td>
                        <td>
                        Foundations: Optimization, Learning Theory, Overfitting, Bias/variance Tradeoff, Evaluation 
                        </td>
                    </tr>
                    <tr class="active">
                        <td>7-8
                        </td>
                        <td>
                        Supervised Learning, Part II: Kernel Methods, Boosting, Neural Networks
                        </td>
                    </tr>
                    <tr class="success">
                        <td>9-10</td>
                        <td>Unsupervised Learning: PCA, Clustering, EM
                        </td>
                    </tr>
                    <tr class="active">
                        <td>11+</td>
                        <td>
                        Additional Topics: HMMs, Graphical Models, Large-scale Machine Learning, Reinforcement Learning, Online Learning, Applications, ...
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        </div>

        <hr>
    </div>
    <!-- /container -->
 

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
    <script src="bootstrap/js/bootstrap.min.js"></script>


</body>
